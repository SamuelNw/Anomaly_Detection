{"cells":[{"cell_type":"markdown","source":["## <u>Anomaly Detection</u>\n","###### After successfully ingesting the data (inside **PesaTransactionsV1 Lakehouse** ), the project flow is divided into the following main steps.\n","\n","1) Train the model, test the data for anomalies.\n","2) Use OpenAI GPT4o-mini to analyze the anomaly level (low/medium/high), along with explanations to the deductions and the message that will be sent to the risk analysts.\n","3) Connect to Azure blob storage, upload `.txt` files (the text in here is the alert message content with crucial info) for any anomalies found. \n","\n","After the last step, Power Automate takes over from here. We've set up a trigger to listen to addition or modifications of files in the blob storage, and steps to send emails of the alert message to risk analysts."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c63f7bfa-8f50-4632-8ca2-171b7b189a79"},{"cell_type":"markdown","source":["##### **STEP 1:** Train the model, test the data for anomalies."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"95d6cb91-bf3b-45d5-8dfc-2b08412f5095"},{"cell_type":"code","source":["# Read the data and initiate the DataFrame.\n","df = spark.read.table(\"PesaTransactionsV1.PesaTransactions\")\n","df = df.toPandas()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5b49b613-c45b-4d07-8a42-6e2fabb6ef01"},{"cell_type":"code","source":["# Select and prepare relevant features (Acc No, Posting Date, Amount).\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","\n","df[\"AccountNumber\"] = df[\"NewAccountNo\"].str.extract(r\"-(\\d+)-\")\n","df[\"PostingDate\"] = pd.to_datetime(df[\"PostingDate\"])\n","df[\"Day\"] = df[\"PostingDate\"].dt.dayofyear\n","\n","# The amount needs to be normalized\n","df[\"Scaled_Amount\"] = scaler.fit_transform(df[[\"Amount\"]])\n","\n","# Determine the features\n","features = df[[\"AccountNumber\", \"Scaled_Amount\", \"Day\"]]"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":18,"statement_ids":[18],"state":"finished","livy_statement_state":"available","session_id":"b23c15c2-45ae-48ff-b5d8-e1dfcd7a81be","normalized_state":"finished","queued_time":"2025-04-07T21:12:54.622444Z","session_start_time":null,"execution_start_time":"2025-04-07T21:12:54.6235697Z","execution_finish_time":"2025-04-07T21:12:54.9005654Z","parent_msg_id":"6537cc7e-71b6-43f4-b841-e138fe31ee8c"},"text/plain":"StatementMeta(, b23c15c2-45ae-48ff-b5d8-e1dfcd7a81be, 18, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d3d64748-0d07-4942-a104-824a6f1984df"},{"cell_type":"code","source":["# Split the data for testing and evaluation.\n","from sklearn.model_selection import train_test_split\n","\n","train_data, test_data = train_test_split(features, test_size=0.005, random_state=42)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":19,"statement_ids":[19],"state":"finished","livy_statement_state":"available","session_id":"b23c15c2-45ae-48ff-b5d8-e1dfcd7a81be","normalized_state":"finished","queued_time":"2025-04-07T21:12:58.2744405Z","session_start_time":null,"execution_start_time":"2025-04-07T21:12:58.2757137Z","execution_finish_time":"2025-04-07T21:12:58.5939291Z","parent_msg_id":"98cecb21-d201-4f7b-beba-916f049e53c6"},"text/plain":"StatementMeta(, b23c15c2-45ae-48ff-b5d8-e1dfcd7a81be, 19, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8eb790d0-9c3d-4c69-81f6-0e437050e40b"},{"cell_type":"code","source":["# Train the the model on the respective data (train_data)\n","\"\"\"\n","- We chose the Isolation Forest Algorithm, for it's strengths in working with high-dimension datasets.\n","\"\"\"\n","\n","from sklearn.ensemble import IsolationForest\n","from sklearn.impute import SimpleImputer\n","\n","imputer = SimpleImputer(strategy='most_frequent')\n","train_data_imputed = imputer.fit_transform(train_data)\n","\n","model = IsolationForest(\n","    n_estimators=100,\n","    contamination=0.05,\n","    bootstrap=False,\n","    verbose=1,\n","    random_state=42\n",")\n","model.fit(train_data_imputed)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":20,"statement_ids":[20],"state":"finished","livy_statement_state":"available","session_id":"b23c15c2-45ae-48ff-b5d8-e1dfcd7a81be","normalized_state":"finished","queued_time":"2025-04-07T21:13:00.8941308Z","session_start_time":null,"execution_start_time":"2025-04-07T21:13:00.8952441Z","execution_finish_time":"2025-04-07T21:13:08.8018501Z","parent_msg_id":"bba42cf9-6b17-45db-b043-a7fe15eb2876"},"text/plain":"StatementMeta(, b23c15c2-45ae-48ff-b5d8-e1dfcd7a81be, 20, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"]},{"output_type":"stream","name":"stderr","text":["[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n"]},{"output_type":"display_data","data":{"application/vnd.mlflow.run-widget+json":{"info":{"artifact_uri":"sds://onelakesouthafricanorth.pbidedicated.windows.net/b6ab2d30-caf0-43d2-ad6a-165345f8b178/63c9ffa7-d6ff-4d7c-b6c9-f84fca68a44c/4397a685-007e-4921-90d4-ae989b397599/artifacts","end_time":1744060386,"experiment_id":"8ac12706-f04e-4513-9fc6-f2d5877224d0","lifecycle_stage":"active","run_id":"4397a685-007e-4921-90d4-ae989b397599","run_name":"quiet_pea_7xbtykyy","run_uuid":"4397a685-007e-4921-90d4-ae989b397599","start_time":1744060381,"status":"FINISHED","user_id":"561e81b0-4da1-42c1-b290-381c5782272c"},"data":{"metrics":{},"params":{"bootstrap":"False","contamination":"0.05","max_features":"1.0","max_samples":"auto","n_estimators":"100","n_jobs":"None","random_state":"42","verbose":"1","warm_start":"False"},"tags":{"mlflow.user":"dcaf7b03-32e1-45bc-b03b-2da2786f6cfd","synapseml.notebook.artifactId":"92760d45-1bd0-4597-be48-41637b6bb165","synapseml.user.name":"Samuel Wanyoike","synapseml.user.id":"c78ab9c3-b06e-46c4-b894-a5dbbaa43d98","synapseml.livy.id":"b23c15c2-45ae-48ff-b5d8-e1dfcd7a81be","mlflow.autologging":"sklearn","estimator_name":"IsolationForest","estimator_class":"sklearn.ensemble._iforest.IsolationForest","mlflow.rootRunId":"4397a685-007e-4921-90d4-ae989b397599","mlflow.runName":"quiet_pea_7xbtykyy","synapseml.experimentName":"CleanAndTestData","synapseml.experiment.artifactId":"63c9ffa7-d6ff-4d7c-b6c9-f84fca68a44c"}},"inputs":{"dataset_inputs":[]}}},"metadata":{}},{"output_type":"execute_result","execution_count":59,"data":{"text/plain":"IsolationForest(contamination=0.05, random_state=42, verbose=1)","text/html":"<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IsolationForest(contamination=0.05, random_state=42, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IsolationForest</label><div class=\"sk-toggleable__content\"><pre>IsolationForest(contamination=0.05, random_state=42, verbose=1)</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"781b4ff9-02c0-4ea0-b770-0a892fdab09f"},{"cell_type":"code","source":["# Detecting anomalies in the data\n","from sklearn.impute import SimpleImputer\n","\n","imputer = SimpleImputer(strategy='most_frequent')\n","test_data_imputed = imputer.fit_transform(test_data)\n","\n","test_data[\"Status\"] = model.predict(test_data_imputed)\n","test_data[\"Status\"] = test_data[\"Status\"].map({1: \"Normal\", -1: \"Anomaly\"})"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":21,"statement_ids":[21],"state":"finished","livy_statement_state":"available","session_id":"b23c15c2-45ae-48ff-b5d8-e1dfcd7a81be","normalized_state":"finished","queued_time":"2025-04-07T21:13:12.2937383Z","session_start_time":null,"execution_start_time":"2025-04-07T21:13:12.2948952Z","execution_finish_time":"2025-04-07T21:13:12.5935514Z","parent_msg_id":"e921ed96-9350-4419-90e7-00d789d986b4"},"text/plain":"StatementMeta(, b23c15c2-45ae-48ff-b5d8-e1dfcd7a81be, 21, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"86c56c6d-d99f-4f1f-81e1-5ba6facaaba2"},{"cell_type":"code","source":["# Combine with original data and save to lakehouse\n","\"\"\"\n","- We'll create a new table named \"Anomaly_Results_v2\" that will have the original values, \n","and an added column of status where the status value for each row will be either \"Normal\" or \"Anomaly\".\n","\"\"\"\n","from pyspark.sql.functions import col\n","\n","result_df = test_data.copy()\n","result_df[\"Amount\"] = df.loc[result_df.index, \"Amount\"]\n","\n","spark_df = spark.createDataFrame(result_df)\n","spark_df = spark_df.withColumn(\"Amount\", col(\"Amount\").cast(\"int\"))\n","spark_df.write \\\n","    .mode(\"overwrite\") \\\n","    .option(\"overwriteSchema\", \"true\") \\\n","    .saveAsTable(\"PesaTransactionsV1.Anomaly_Results_v2\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":22,"statement_ids":[22],"state":"finished","livy_statement_state":"available","session_id":"b23c15c2-45ae-48ff-b5d8-e1dfcd7a81be","normalized_state":"finished","queued_time":"2025-04-07T21:13:16.3558844Z","session_start_time":null,"execution_start_time":"2025-04-07T21:13:16.3570083Z","execution_finish_time":"2025-04-07T21:13:20.3792077Z","parent_msg_id":"dcf562e4-0bc5-49c4-a6a6-0a4442a95d93"},"text/plain":"StatementMeta(, b23c15c2-45ae-48ff-b5d8-e1dfcd7a81be, 22, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b67a0371-63ba-4ef6-a435-4c6c7876e21e"},{"cell_type":"code","source":["# Filter out anomaly rows\n","import pandas as pd\n","\n","df_anomalies = spark.read.table(\"Anomaly_Results_v2\").filter(\"Status = 'Anomaly'\").toPandas()\n","df_anomalies.head()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":23,"statement_ids":[23],"state":"finished","livy_statement_state":"available","session_id":"b23c15c2-45ae-48ff-b5d8-e1dfcd7a81be","normalized_state":"finished","queued_time":"2025-04-07T21:13:24.7064454Z","session_start_time":null,"execution_start_time":"2025-04-07T21:13:24.7076318Z","execution_finish_time":"2025-04-07T21:13:28.2482655Z","parent_msg_id":"d94a5f94-0f4d-446f-8c88-ecd784b2e404"},"text/plain":"StatementMeta(, b23c15c2-45ae-48ff-b5d8-e1dfcd7a81be, 23, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":68,"data":{"text/plain":"  AccountNumber  Scaled_Amount  Day   Status  Amount\n0         97346       2.392144  128  Anomaly   30000\n1         90346       2.392144  310  Anomaly   30000\n2         09202       2.392144  177  Anomaly   30000\n3         86024       2.392144   84  Anomaly   30000\n4         17388       1.391652    2  Anomaly   21000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AccountNumber</th>\n      <th>Scaled_Amount</th>\n      <th>Day</th>\n      <th>Status</th>\n      <th>Amount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>97346</td>\n      <td>2.392144</td>\n      <td>128</td>\n      <td>Anomaly</td>\n      <td>30000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>90346</td>\n      <td>2.392144</td>\n      <td>310</td>\n      <td>Anomaly</td>\n      <td>30000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>09202</td>\n      <td>2.392144</td>\n      <td>177</td>\n      <td>Anomaly</td>\n      <td>30000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>86024</td>\n      <td>2.392144</td>\n      <td>84</td>\n      <td>Anomaly</td>\n      <td>30000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>17388</td>\n      <td>1.391652</td>\n      <td>2</td>\n      <td>Anomaly</td>\n      <td>21000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"738e582c-66a9-4c33-8266-9d3523c37715"},{"cell_type":"markdown","source":["##### **STEP 2:** Use OpenAI GPT4o-mini to analyze the anomaly level (low/medium/high), along with explanations to the deductions and the message that will be sent to the risk analysts.\n","\n","- At this point, we've already created a Key Vault on the Azure portal to store the OpenAI endpoint-url and the OpenAI key."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cfe25fed-4d98-4c73-a284-bb9aa9381039"},{"cell_type":"code","source":["# Function to generate the prompt to send to GPT4 for each anomaly\n","def format_prompt(transaction):\n","    return f\"\"\"\n","        A transaction has been flagged by an anomaly detection system. \n","        Details:\n","        - Anomaly: {transaction[\"Status\"]}\n","        - Amount: KES{transaction[\"Amount\"]}\n","        - Account No: {transaction[\"AccountNumber\"]}\n","\n","        Evaluate this transaction and assign a risk level (low, medium, high) with\n","        a short explanation. Provide a final alert message in plain English for a \n","        fraud analyst.\n","\n","        Add three columns for your output:\n","        1. RiskLevel.\n","        2. Explanation.\n","        3. AlertMessage\n","    \"\"\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":24,"statement_ids":[24],"state":"finished","livy_statement_state":"available","session_id":"b23c15c2-45ae-48ff-b5d8-e1dfcd7a81be","normalized_state":"finished","queued_time":"2025-04-07T21:13:35.8445296Z","session_start_time":null,"execution_start_time":"2025-04-07T21:13:35.8456148Z","execution_finish_time":"2025-04-07T21:13:36.1449608Z","parent_msg_id":"e5748986-b878-47f9-a2aa-6f56282311d3"},"text/plain":"StatementMeta(, b23c15c2-45ae-48ff-b5d8-e1dfcd7a81be, 24, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e14d64cf-f46f-40ed-8497-72fee74f9ff0"},{"cell_type":"code","source":["# Install the packages below (the versions are just as crucial)\n","!pip install openai==0.28.1 typing-extensions==4.5.0 pydantic==1.10.13 notebookutils"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b889684c-047f-4a52-a7b0-f3fdd51bc2cb"},{"cell_type":"code","source":["# Call GPT4o mini\n","import openai\n","from notebookutils.mssparkutils.credentials import getSecret\n","import re\n","import pandas as pd\n","import json\n","\n","\n","# Constants\n","KEY_VAULT_ENDPOINT = \"https://pesatransactionskeyvault.vault.azure.net/\"\n","AZURE_OPENAI_API_KEY = getSecret(KEY_VAULT_ENDPOINT, \"openaiGPT4o-mini-key\")\n","AZURE_OPENAI_ENDPOINT_URL = getSecret(KEY_VAULT_ENDPOINT, \"openaiendpointURL\")\n","\n","openai.api_type = \"azure\"\n","openai.api_base = AZURE_OPENAI_ENDPOINT_URL\n","openai.api_version = \"2025-03-01-preview\"\n","openai.api_key = AZURE_OPENAI_API_KEY\n","\n","def gpt4o_risk_evaluation(prompt):\n","    response = openai.ChatCompletion.create(\n","        engine=\"gpt-4o-mini-kenya-hack\",\n","        messages=[\n","            {\"role\": \"system\", \"content\": \"\"\"You are a fraud assistant. Always respond in valid JSON format with these exact keys:\n","            {\n","                \"risk_level\": \"low/medium/high\",\n","                \"explanation\": \"your explanation here\",\n","                \"alert_message\": \"your alert message here\"\n","            }\"\"\"},\n","                        {\"role\": \"user\", \"content\": prompt}\n","                    ],\n","        response_format={\"type\": \"json_object\"},\n","        temperature=0.7\n","    )\n","    return json.loads(response.choices[0].message[\"content\"])\n","\n","\n","\n","def parse_gpt4_response(response_text):\n","    \"\"\"Parse the GPT-4 response into structured components\"\"\"\n","    risk_level = response_text[\"risk_level\"]\n","    explanation = response_text[\"explanation\"]\n","    alert_message = response_text[\"alert_message\"]\n","    \n","    return pd.Series({\n","        'RiskLevel': risk_level,\n","        'Explanation': explanation,\n","        'AlertMessage': alert_message\n","    })"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":26,"statement_ids":[26],"state":"finished","livy_statement_state":"available","session_id":"b23c15c2-45ae-48ff-b5d8-e1dfcd7a81be","normalized_state":"finished","queued_time":"2025-04-07T21:14:15.2348828Z","session_start_time":null,"execution_start_time":"2025-04-07T21:14:15.2359931Z","execution_finish_time":"2025-04-07T21:14:17.568535Z","parent_msg_id":"bc851001-1f3e-43ce-9a65-966b1a8f8e1e"},"text/plain":"StatementMeta(, b23c15c2-45ae-48ff-b5d8-e1dfcd7a81be, 26, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4ee9d58d-8acf-4b96-b32d-4e46be069934"},{"cell_type":"code","source":["# Loop through the anomalies and evaluate each\n","\"\"\"\n","- The intention for this is to come up with a table with the original data, but with additional columns for the risk level,\n","the explanation as to why this is an anomaly, and the alert message to be sent to analysts.\n","\"\"\"\n","\n","response_series = df_anomalies.apply(\n","    lambda row: parse_gpt4_response(gpt4o_risk_evaluation(format_prompt(row))), \n","    axis=1\n",")\n","\n","# Add the parsed columns to your DataFrame\n","df_anomalies[['RiskLevel', 'Explanation', 'AlertMessage']] = response_series\n","\n","# Show what we have so far:\n","df_anomalies.head()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":27,"statement_ids":[27],"state":"finished","livy_statement_state":"available","session_id":"b23c15c2-45ae-48ff-b5d8-e1dfcd7a81be","normalized_state":"finished","queued_time":"2025-04-07T21:14:23.5187007Z","session_start_time":null,"execution_start_time":"2025-04-07T21:14:23.5199068Z","execution_finish_time":"2025-04-07T21:14:39.4441836Z","parent_msg_id":"4dbfa3fa-8ac4-447f-99bb-35ac066fad5a"},"text/plain":"StatementMeta(, b23c15c2-45ae-48ff-b5d8-e1dfcd7a81be, 27, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":80,"data":{"text/plain":"  AccountNumber  Scaled_Amount  Day   Status  Amount RiskLevel  \\\n0         97346       2.392144  128  Anomaly   30000    medium   \n1         90346       2.392144  310  Anomaly   30000    medium   \n2         09202       2.392144  177  Anomaly   30000    medium   \n3         86024       2.392144   84  Anomaly   30000    medium   \n4         17388       1.391652    2  Anomaly   21000    medium   \n\n                                         Explanation  \\\n0  The transaction amount of KES30000 is signific...   \n1  The transaction amount of KES30000 is signific...   \n2  The transaction amount of KES30000 is signific...   \n3  The transaction amount of KES30000 is notable ...   \n4  The transaction amount of KES21000 is signific...   \n\n                                        AlertMessage  \n0  Transaction flagged for review: KES30000 to ac...  \n1  A transaction of KES30000 has been flagged as ...  \n2  Transaction flagged for review: KES30000 on ac...  \n3  Transaction of KES30000 on Account No: 86024 h...  \n4  The transaction of KES21000 from account numbe...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AccountNumber</th>\n      <th>Scaled_Amount</th>\n      <th>Day</th>\n      <th>Status</th>\n      <th>Amount</th>\n      <th>RiskLevel</th>\n      <th>Explanation</th>\n      <th>AlertMessage</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>97346</td>\n      <td>2.392144</td>\n      <td>128</td>\n      <td>Anomaly</td>\n      <td>30000</td>\n      <td>medium</td>\n      <td>The transaction amount of KES30000 is signific...</td>\n      <td>Transaction flagged for review: KES30000 to ac...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>90346</td>\n      <td>2.392144</td>\n      <td>310</td>\n      <td>Anomaly</td>\n      <td>30000</td>\n      <td>medium</td>\n      <td>The transaction amount of KES30000 is signific...</td>\n      <td>A transaction of KES30000 has been flagged as ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>09202</td>\n      <td>2.392144</td>\n      <td>177</td>\n      <td>Anomaly</td>\n      <td>30000</td>\n      <td>medium</td>\n      <td>The transaction amount of KES30000 is signific...</td>\n      <td>Transaction flagged for review: KES30000 on ac...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>86024</td>\n      <td>2.392144</td>\n      <td>84</td>\n      <td>Anomaly</td>\n      <td>30000</td>\n      <td>medium</td>\n      <td>The transaction amount of KES30000 is notable ...</td>\n      <td>Transaction of KES30000 on Account No: 86024 h...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>17388</td>\n      <td>1.391652</td>\n      <td>2</td>\n      <td>Anomaly</td>\n      <td>21000</td>\n      <td>medium</td>\n      <td>The transaction amount of KES21000 is signific...</td>\n      <td>The transaction of KES21000 from account numbe...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fd2286fd-10b6-4218-85a1-c1bb853de529"},{"cell_type":"code","source":["# Save results back to the lakehouse but in a new table\n","from pyspark.sql import SparkSession\n","\n","spark_df = spark.createDataFrame(df_anomalies)\n","\n","spark_df.write.format(\"delta\") \\\n","    .option(\"mergeSchema\", \"true\") \\\n","    .mode(\"append\") \\\n","    .saveAsTable(\"Anomaly_Risk_Summary_v2\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":28,"statement_ids":[28],"state":"finished","livy_statement_state":"available","session_id":"b23c15c2-45ae-48ff-b5d8-e1dfcd7a81be","normalized_state":"finished","queued_time":"2025-04-07T21:15:04.0255896Z","session_start_time":null,"execution_start_time":"2025-04-07T21:15:04.0268134Z","execution_finish_time":"2025-04-07T21:15:08.7480525Z","parent_msg_id":"d180fe50-7d79-4e68-83c9-5ba3c1bded02"},"text/plain":"StatementMeta(, b23c15c2-45ae-48ff-b5d8-e1dfcd7a81be, 28, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e0147be3-9a72-4f78-ab4d-758d40739541"},{"cell_type":"markdown","source":["##### **STEP 3:** Connect to Azure blob storage, upload `.txt` files (the text in here is the alert message content with crucial info) for any anomalies found.\n","\n","- At this point, a Storage Account is already created on Azure, along with the blob container that we'll use to store the uploaded files.\n","- The final cell in this section will log; A message to show if connecting to the blob container is successful, if uploading the anomaly files is successful, and lastly, a verification to show how many items are currently in the blob container (well, only a few file names)."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e117c81e-b157-46d9-9440-73b278c001d4"},{"cell_type":"code","source":["!pip install azure-storage-blob"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9f1c2399-3e7b-4372-a361-c73d0782bdbe"},{"cell_type":"code","source":["## Save Anomalies Summary to Blob Storage\n","# This cell handles exporting our processed data to Azure Blob Storage as JSON\n","\n","# %%\n","import pandas as pd\n","from azure.storage.blob import BlobServiceClient\n","from datetime import datetime\n","import json\n","import os\n","\n","# Constants\n","KEY_VAULT_ENDPOINT = \"https://pesatransactionskeyvault.vault.azure.net/\"\n","AZURE_OPENAI_API_KEY = getSecret(KEY_VAULT_ENDPOINT, \"openaiGPT4o-mini-key\")\n","AZURE_OPENAI_ENDPOINT_URL = getSecret(KEY_VAULT_ENDPOINT, \"openaiendpointURL\")\n","\n","# Configuration Section\n","STORAGE_ACCOUNT_NAME = getSecret(KEY_VAULT_ENDPOINT, \"storageAccountName\")\n","STORAGE_ACCOUNT_KEY = getSecret(KEY_VAULT_ENDPOINT, \"storageAccountKey\")\n","CONTAINER_NAME = \"gpt4osummary\"\n","\n","# Generate timestamp for filename\n","TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n","BLOB_NAME = f\"anomaly_alerts_{TIMESTAMP}.txt\"\n","\n","# Create the connection to Azure Blob Storage\n","try:\n","    connection_string = f\"DefaultEndpointsProtocol=https;AccountName={STORAGE_ACCOUNT_NAME};\\\n","        AccountKey={STORAGE_ACCOUNT_KEY};EndpointSuffix=core.windows.net\"\n","    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n","    \n","    # Get reference to container (will create if doesn't exist)\n","    container_client = blob_service_client.get_container_client(CONTAINER_NAME)\n","    if not container_client.exists():\n","        container_client.create_container()\n","    \n","    print(f\"Successfully connected to Azure Blob Storage container: {CONTAINER_NAME}\")\n","except Exception as e:\n","    print(f\"Error connecting to Azure Blob Storage: {str(e)}\")\n","    raise\n","\n","# %%\n","# Convert Spark DataFrame to text files and upload\n","try:\n","    # First convert Spark DataFrame to Pandas\n","    pandas_df = spark_df.toPandas()\n","    \n","    # Ensure the required column exists\n","    if 'AlertMessage' not in pandas_df.columns:\n","        raise ValueError(\"DataFrame is missing required 'AlertMessage' column\")\n","    \n","    # Upload each summary as a separate text file\n","    for index, row in pandas_df.iterrows():\n","        # Create filename with timestamp and index\n","        txt_filename = f\"anomaly_summary_{TIMESTAMP}_{index}.txt\"\n","        \n","        # Get the summary text\n","        summary_text = str(row['AlertMessage'])\n","        \n","        # Upload to blob storage\n","        blob_client = container_client.get_blob_client(txt_filename)\n","        blob_client.upload_blob(summary_text, overwrite=True)\n","        \n","        print(f\"Uploaded: {txt_filename} (Size: {len(summary_text)} characters)\")\n","    \n","    print(f\"\\nSuccessfully uploaded {len(pandas_df)} text files to container {CONTAINER_NAME}\")\n","\n","except Exception as e:\n","    print(f\"Error uploading text files to blob storage: {str(e)}\")\n","    raise\n","\n","# Verification - list blobs in container (optional)\n","print(\"\\nCurrent blobs in container:\")\n","try:\n","    blob_list = container_client.list_blobs()\n","    txt_files = [blob for blob in blob_list if blob.name.endswith('.txt')]\n","    \n","    print(f\"Found {len(txt_files)} text files:\")\n","    for blob in txt_files[:5]:\n","        print(f\"- {blob.name} (Size: {blob.size} bytes)\")\n","    if len(txt_files) > 5:\n","        print(f\"- ... and {len(txt_files)-5} more\")\n","    \n","except Exception as e:\n","    print(f\"Error listing blobs: {str(e)}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4692cd26-5f62-4f0a-8bf8-8bd39be9cde2"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en","ms_ignore_dictionary":["lakehouse","openai"]}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"04ad2f29-2adf-4cb3-891c-174254ca83a8"}],"default_lakehouse":"04ad2f29-2adf-4cb3-891c-174254ca83a8","default_lakehouse_name":"PesaTransactionsV1","default_lakehouse_workspace_id":"b6ab2d30-caf0-43d2-ad6a-165345f8b178"}}},"nbformat":4,"nbformat_minor":5}