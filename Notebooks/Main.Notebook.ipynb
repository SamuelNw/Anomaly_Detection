{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df1007c4-8047-484a-a6cd-60fdaa36f626",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## <u>Anomaly Detection</u>\n",
    "###### After successfully ingesting the data (inside **PesaTransactionsV1 Lakehouse** ), the project flow is divided into the following main steps.\n",
    "\n",
    "1) Train the model, test the data for anomalies.\n",
    "2) Use OpenAI GPT4o-mini to analyze the anomaly level (low/medium/high), along with explanations to the deductions and the message that will be sent to the risk analysts.\n",
    "3) Connect to Azure blob storage, upload `.txt` files (the text in here is the alert message content with crucial info) for any anomalies found. \n",
    "\n",
    "After the last step, Power Automate takes over from here. We've set up a trigger to listen to addition or modifications of files in the blob storage, and steps to send emails of the alert message to risk analysts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16081a1-c7f1-4e9b-9d84-aa8e6e8ffab8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### **STEP 1:** Train the model, test the data for anomalies.\n",
    "\n",
    "###### Import Notes:\n",
    "- The data features that are selected to be used in the machine learning model include; \n",
    "    - **Account Numbers** - Unique identifiers.\n",
    "    - **Transaction Posting Date** - The day of the transaction.\n",
    "    - **Transaction Time** - Exact times they happened.\n",
    "    - **Transaction Amount**\n",
    "    - **Rolling Average** - The mean of the total amount transacted by each account number/user.\n",
    "    - **Previous Transaction Time** - The last time a certain user transacted.\n",
    "    - **Is Transaction the first** - Whether this is the first transaction for a particular user.\n",
    "    - **Time since last transaction** - Time difference between the last and second last transactions for each user/account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c870359c-f98c-4b37-ab01-95f9702baacf",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-04-08T19:48:44.869267Z",
       "execution_start_time": "2025-04-08T19:48:43.9934361Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "e38e7510-2ab7-499f-a79d-375398398961",
       "queued_time": "2025-04-08T19:48:43.9921848Z",
       "session_id": "ac3cf0cc-e561-4c55-aa92-182d3645a533",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 68,
       "statement_ids": [
        68
       ]
      },
      "text/plain": [
       "StatementMeta(, ac3cf0cc-e561-4c55-aa92-182d3645a533, 68, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AccountNo</th>\n",
       "      <th>Time</th>\n",
       "      <th>Amount</th>\n",
       "      <th>PostingDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XX-01400-XX</td>\n",
       "      <td>13:29:01</td>\n",
       "      <td>1000</td>\n",
       "      <td>10/11/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XX-93316-XX</td>\n",
       "      <td>5:40:57</td>\n",
       "      <td>20000</td>\n",
       "      <td>10/12/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XX-06984-XX</td>\n",
       "      <td>4:23:17</td>\n",
       "      <td>5000</td>\n",
       "      <td>10/13/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XX-57152-XX</td>\n",
       "      <td>7:45:45</td>\n",
       "      <td>10000</td>\n",
       "      <td>10/13/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XX-91706-XX</td>\n",
       "      <td>14:17:58</td>\n",
       "      <td>7000</td>\n",
       "      <td>10/13/2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     AccountNo      Time  Amount PostingDate\n",
       "0  XX-01400-XX  13:29:01    1000  10/11/2018\n",
       "1  XX-93316-XX   5:40:57   20000  10/12/2018\n",
       "2  XX-06984-XX   4:23:17    5000  10/13/2018\n",
       "3  XX-57152-XX   7:45:45   10000  10/13/2018\n",
       "4  XX-91706-XX  14:17:58    7000  10/13/2018"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data and initiate the DataFrame.\n",
    "\"\"\"\n",
    "- For this, there are two ways; \n",
    "    - the first being reading from the table \"PesaTransactions\".\n",
    "    - the second being reading it from the csv file \"PesaTransactionsToday.csv\"\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "# df = spark.read.table(\"PesaTransactionsV1.PesaTransactions\")\n",
    "# df = df.toPandas()\n",
    "\n",
    "df = pd.read_csv('/lakehouse/default/Files/PesaTransactionsToday.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1b0853c8-7b7c-4ebb-96aa-39213f9b909f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-04-08T19:48:45.7112559Z",
       "execution_start_time": "2025-04-08T19:48:44.8711044Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "e4722442-02c1-482d-ba5d-ffbfb20025ae",
       "queued_time": "2025-04-08T19:48:44.0708386Z",
       "session_id": "ac3cf0cc-e561-4c55-aa92-182d3645a533",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 69,
       "statement_ids": [
        69
       ]
      },
      "text/plain": [
       "StatementMeta(, ac3cf0cc-e561-4c55-aa92-182d3645a533, 69, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AccountNo</th>\n",
       "      <th>Time</th>\n",
       "      <th>Amount</th>\n",
       "      <th>PostingDate</th>\n",
       "      <th>AccountNumber</th>\n",
       "      <th>datetime</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>txn_count</th>\n",
       "      <th>rolling_avg_amt</th>\n",
       "      <th>prev_txn_time</th>\n",
       "      <th>is_first_txn</th>\n",
       "      <th>time_since_last</th>\n",
       "      <th>log_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38926</th>\n",
       "      <td>XX-00238-XX</td>\n",
       "      <td>14:32:44</td>\n",
       "      <td>5000</td>\n",
       "      <td>11/24/2021</td>\n",
       "      <td>00238</td>\n",
       "      <td>2021-11-24 14:32:44</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8.517393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39756</th>\n",
       "      <td>XX-00238-XX</td>\n",
       "      <td>10:25:56</td>\n",
       "      <td>1000</td>\n",
       "      <td>12/3/2021</td>\n",
       "      <td>00238</td>\n",
       "      <td>2021-12-03 10:25:56</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>2021-11-24 14:32:44</td>\n",
       "      <td>0</td>\n",
       "      <td>762792.0</td>\n",
       "      <td>6.908755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39758</th>\n",
       "      <td>XX-00238-XX</td>\n",
       "      <td>10:27:38</td>\n",
       "      <td>26000</td>\n",
       "      <td>12/3/2021</td>\n",
       "      <td>00238</td>\n",
       "      <td>2021-12-03 10:27:38</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10666.666667</td>\n",
       "      <td>2021-12-03 10:25:56</td>\n",
       "      <td>0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>10.165890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35766</th>\n",
       "      <td>XX-00402-XX</td>\n",
       "      <td>17:36:44</td>\n",
       "      <td>20000</td>\n",
       "      <td>10/19/2021</td>\n",
       "      <td>00402</td>\n",
       "      <td>2021-10-19 17:36:44</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.903538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35767</th>\n",
       "      <td>XX-00402-XX</td>\n",
       "      <td>17:38:32</td>\n",
       "      <td>3000</td>\n",
       "      <td>10/19/2021</td>\n",
       "      <td>00402</td>\n",
       "      <td>2021-10-19 17:38:32</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11500.000000</td>\n",
       "      <td>2021-10-19 17:36:44</td>\n",
       "      <td>0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>8.006701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         AccountNo      Time  Amount PostingDate AccountNumber  \\\n",
       "38926  XX-00238-XX  14:32:44    5000  11/24/2021         00238   \n",
       "39756  XX-00238-XX  10:25:56    1000   12/3/2021         00238   \n",
       "39758  XX-00238-XX  10:27:38   26000   12/3/2021         00238   \n",
       "35766  XX-00402-XX  17:36:44   20000  10/19/2021         00402   \n",
       "35767  XX-00402-XX  17:38:32    3000  10/19/2021         00402   \n",
       "\n",
       "                 datetime  hour  day_of_week  day  month  is_weekend  \\\n",
       "38926 2021-11-24 14:32:44    14            2   24     11           0   \n",
       "39756 2021-12-03 10:25:56    10            4    3     12           0   \n",
       "39758 2021-12-03 10:27:38    10            4    3     12           0   \n",
       "35766 2021-10-19 17:36:44    17            1   19     10           0   \n",
       "35767 2021-10-19 17:38:32    17            1   19     10           0   \n",
       "\n",
       "       txn_count  rolling_avg_amt       prev_txn_time  is_first_txn  \\\n",
       "38926          3      5000.000000                 NaT             1   \n",
       "39756          3      3000.000000 2021-11-24 14:32:44             0   \n",
       "39758          3     10666.666667 2021-12-03 10:25:56             0   \n",
       "35766          3     20000.000000                 NaT             1   \n",
       "35767          3     11500.000000 2021-10-19 17:36:44             0   \n",
       "\n",
       "       time_since_last  log_amount  \n",
       "38926             -1.0    8.517393  \n",
       "39756         762792.0    6.908755  \n",
       "39758            102.0   10.165890  \n",
       "35766             -1.0    9.903538  \n",
       "35767            108.0    8.006701  "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select and prepare the relevant features:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df[\"AccountNumber\"] = df[\"AccountNo\"].str.extract(r\"-(\\d+)-\")\n",
    "df['datetime'] = pd.to_datetime(df['PostingDate'] + ' ' + df['Time'])\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day_of_week'] = df['datetime'].dt.dayofweek  # 0=Monday\n",
    "df['day'] = df['datetime'].dt.day\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Total number of transactions per member\n",
    "member_txn_count = df.groupby('AccountNumber')['Amount'].count().rename('txn_count')\n",
    "\n",
    "# Merge back\n",
    "df = df.merge(member_txn_count, on='AccountNumber', how='left')\n",
    "df = df.sort_values(by=['AccountNumber', 'datetime'])\n",
    "\n",
    "# Rolling average amount per member (last 3 transactions)\n",
    "df['rolling_avg_amt'] = df.groupby('AccountNumber')['Amount'].rolling(window=3, min_periods=1).mean().reset_index(0, drop=True)\n",
    "df['prev_txn_time'] = df.groupby('AccountNumber')['datetime'].shift(1)\n",
    "df['is_first_txn'] = df['prev_txn_time'].isna().astype(int)\n",
    "df['time_since_last'] = (df['datetime'] - df['prev_txn_time']).dt.total_seconds()\n",
    "df['time_since_last'].fillna(-1, inplace=True)  # Use -1 or a special value\n",
    "df['log_amount'] = np.log1p(df['Amount'])  # handles zero safely\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b05839fc-8304-4bb4-9367-6be1d5c7ce4a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-04-08T19:48:46.0339118Z",
       "execution_start_time": "2025-04-08T19:48:45.7130118Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "b2ea183f-6d01-487b-ad96-17db03f0905d",
       "queued_time": "2025-04-08T19:48:44.1469706Z",
       "session_id": "ac3cf0cc-e561-4c55-aa92-182d3645a533",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 70,
       "statement_ids": [
        70
       ]
      },
      "text/plain": [
       "StatementMeta(, ac3cf0cc-e561-4c55-aa92-182d3645a533, 70, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      AccountNumber  AccountNumberEncoded\n",
      "38926         00238                     0\n",
      "39756         00238                     0\n",
      "39758         00238                     0\n",
      "35766         00402                     1\n",
      "35767         00402                     1\n"
     ]
    }
   ],
   "source": [
    "# Determine the features\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df['AccountNumberEncoded'] = encoder.fit_transform(df['AccountNumber'])\n",
    "\n",
    "feature_cols = [\n",
    "    'log_amount',\n",
    "    'AccountNumberEncoded',\n",
    "    'hour',\n",
    "    'day_of_week',\n",
    "    'is_weekend',\n",
    "    'txn_count',\n",
    "    'rolling_avg_amt',\n",
    "    'time_since_last',\n",
    "    'is_first_txn'\n",
    "]\n",
    "\n",
    "# Subset features\n",
    "features = df[feature_cols]\n",
    "\n",
    "# Verify the new column is added\n",
    "print(df[['AccountNumber', 'AccountNumberEncoded']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1fbd52-085b-49e7-b293-810db0d3d8bb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-04-08T19:48:46.3167321Z",
       "execution_start_time": "2025-04-08T19:48:46.035821Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "0af7c309-b55c-4e2d-9528-38a4d8546411",
       "queued_time": "2025-04-08T19:48:44.2718115Z",
       "session_id": "ac3cf0cc-e561-4c55-aa92-182d3645a533",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 71,
       "statement_ids": [
        71
       ]
      },
      "text/plain": [
       "StatementMeta(, ac3cf0cc-e561-4c55-aa92-182d3645a533, 71, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split data for training and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(features, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "#X_scaled = scaler.fit_transform(features)\n",
    "X_train_scaled = scaler.fit_transform(train_data)\n",
    "X_test_scaled = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3155fcfd-595f-4712-abee-69e0c6939c10",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-04-08T19:48:54.1448139Z",
       "execution_start_time": "2025-04-08T19:48:46.3187015Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "eacd5137-c34d-4165-ad46-d5b5a7b51615",
       "queued_time": "2025-04-08T19:48:44.3617373Z",
       "session_id": "ac3cf0cc-e561-4c55-aa92-182d3645a533",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 72,
       "statement_ids": [
        72
       ]
      },
      "text/plain": [
       "StatementMeta(, ac3cf0cc-e561-4c55-aa92-182d3645a533, 72, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.mlflow.run-widget+json": {
       "data": {
        "metrics": {},
        "params": {
         "bootstrap": "False",
         "contamination": "0.01",
         "max_features": "1.0",
         "max_samples": "auto",
         "n_estimators": "100",
         "n_jobs": "None",
         "random_state": "42",
         "verbose": "0",
         "warm_start": "False"
        },
        "tags": {
         "estimator_class": "sklearn.ensemble._iforest.IsolationForest",
         "estimator_name": "IsolationForest",
         "mlflow.autologging": "sklearn",
         "mlflow.rootRunId": "f530fd9d-4f9a-4512-b13c-e3d343837aee",
         "mlflow.runName": "coral_sand_529z9zdb",
         "mlflow.user": "dcaf7b03-32e1-45bc-b03b-2da2786f6cfd",
         "synapseml.experiment.artifactId": "b7302ac9-ed11-4e92-ac9f-ed9531062707",
         "synapseml.experimentName": "Main-Notebook",
         "synapseml.livy.id": "ac3cf0cc-e561-4c55-aa92-182d3645a533",
         "synapseml.notebook.artifactId": "9a20879b-274b-4586-85b9-5e0d0b63a946",
         "synapseml.user.id": "f2193be6-cb3f-4a05-8fd4-304da38ac77e",
         "synapseml.user.name": "Daniel Wangere"
        }
       },
       "info": {
        "artifact_uri": "sds://onelakesouthafricanorth.pbidedicated.windows.net/b6ab2d30-caf0-43d2-ad6a-165345f8b178/b7302ac9-ed11-4e92-ac9f-ed9531062707/f530fd9d-4f9a-4512-b13c-e3d343837aee/artifacts",
        "end_time": 1744141732,
        "experiment_id": "41a3c50c-26b9-4b5a-93f8-82b4dabaa41b",
        "lifecycle_stage": "active",
        "run_id": "f530fd9d-4f9a-4512-b13c-e3d343837aee",
        "run_name": "coral_sand_529z9zdb",
        "run_uuid": "f530fd9d-4f9a-4512-b13c-e3d343837aee",
        "start_time": 1744141726,
        "status": "FINISHED",
        "user_id": "561e81b0-4da1-42c1-b290-381c5782272c"
       },
       "inputs": {
        "dataset_inputs": []
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-14 {color: black;background-color: white;}#sk-container-id-14 pre{padding: 0;}#sk-container-id-14 div.sk-toggleable {background-color: white;}#sk-container-id-14 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-14 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-14 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-14 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-14 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-14 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-14 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-14 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-14 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-14 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-14 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-14 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-14 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-14 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-14 div.sk-item {position: relative;z-index: 1;}#sk-container-id-14 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-14 div.sk-item::before, #sk-container-id-14 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-14 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-14 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-14 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-14 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-14 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-14 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-14 div.sk-label-container {text-align: center;}#sk-container-id-14 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-14 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-14\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IsolationForest(contamination=0.01, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IsolationForest</label><div class=\"sk-toggleable__content\"><pre>IsolationForest(contamination=0.01, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "IsolationForest(contamination=0.01, random_state=42)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Isolation Forest model on the data\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "model = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)\n",
    "model.fit(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3b5b66c5-59b5-4362-b430-4aaa2a5f05e9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-04-08T19:48:54.4224058Z",
       "execution_start_time": "2025-04-08T19:48:54.1468589Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "ffba39aa-d0e0-4ff7-90a4-c5f0049c80a3",
       "queued_time": "2025-04-08T19:48:44.5071473Z",
       "session_id": "ac3cf0cc-e561-4c55-aa92-182d3645a533",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 73,
       "statement_ids": [
        73
       ]
      },
      "text/plain": [
       "StatementMeta(, ac3cf0cc-e561-4c55-aa92-182d3645a533, 73, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict anomalies on the scaled test data\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Convert the predictions (1 = normal, -1 = anomaly) to a more interpretable format\n",
    "y_pred = [1 if pred == 1 else 0 for pred in y_pred]  # 1 = normal, 0 = anomaly\n",
    "\n",
    "# Add predictions to the test set for comparison\n",
    "test_data.loc[:, 'predicted_anomaly'] = y_pred\n",
    "\n",
    "import numpy as np\n",
    "test_data.loc[:, 'Status'] = np.where(test_data['predicted_anomaly'] == 0, 'Anomaly', 'Normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "29b7c12b-5e3d-48fc-8eaf-1d9f0b559c33",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-04-08T19:49:04.2058726Z",
       "execution_start_time": "2025-04-08T19:48:54.4243163Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "6064fb9a-c71d-474a-9e7c-0c29a0434ebe",
       "queued_time": "2025-04-08T19:48:44.6336078Z",
       "session_id": "ac3cf0cc-e561-4c55-aa92-182d3645a533",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 74,
       "statement_ids": [
        74
       ]
      },
      "text/plain": [
       "StatementMeta(, ac3cf0cc-e561-4c55-aa92-182d3645a533, 74, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+----+-----------+----------+---------+------------------+---------------+------------+-----------------+------+------+-------------+----------------------+\n",
      "|        log_amount|AccountNumberEncoded|hour|day_of_week|is_weekend|txn_count|   rolling_avg_amt|time_since_last|is_first_txn|predicted_anomaly|Status|Amount|AccountNumber|AccountNumber_original|\n",
      "+------------------+--------------------+----+-----------+----------+---------+------------------+---------------+------------+-----------------+------+------+-------------+----------------------+\n",
      "|10.308985993422082|                 494|  15|          2|         0|       29|14533.333333333334|      1821633.0|           0|                1|Normal| 30000|        44324|                 44324|\n",
      "| 6.803505257608338|                 848|  10|          1|         0|      136|            1400.0|      2129169.0|           0|                1|Normal|   900|        74804|                 74804|\n",
      "| 7.438971592395862|                 291|  15|          6|         1|      146|            8400.0|      1546538.0|           0|                1|Normal|  1700|        26842|                 26842|\n",
      "| 9.210440366976515|                 548|   8|          0|         0|       77|            6000.0|        68252.0|           0|                1|Normal| 10000|        49302|                 49302|\n",
      "| 9.105090961257085|                 749|   8|          4|         0|       57|3336.6666666666665|       576416.0|           0|                1|Normal|  9000|        66826|                 66826|\n",
      "| 6.685860947068359|                 204|  18|          1|         0|       42| 933.3333333333334|       124639.0|           0|                1|Normal|   800|        20082|                 20082|\n",
      "| 8.006700845440367|                 631|   9|          4|         0|      173|           10260.0|      1549384.0|           0|                1|Normal|  3000|        57152|                 57152|\n",
      "|10.308985993422082|                1068|   0|          2|         0|      201|21633.333333333332|        84319.0|           0|                1|Normal| 30000|        92850|                 92850|\n",
      "|10.308985993422082|                 389|  12|          4|         0|       16|           10250.0|      4148248.0|           0|                1|Normal| 30000|        36396|                 36396|\n",
      "| 6.746412128573374|                 487|  19|          0|         0|      153|1026.6666666666667|       128104.0|           0|                1|Normal|   850|        43818|                 43818|\n",
      "|10.308985993422082|                1111|  18|          0|         0|      145|29233.333333333332|       874708.0|           0|                1|Normal| 30000|        96864|                 96864|\n",
      "|10.308985993422082|                 306|  12|          5|         1|       86|           30000.0|       103014.0|           0|                1|Normal| 30000|        28446|                 28446|\n",
      "| 7.601402334583733|                 723|  13|          6|         1|      155|            2000.0|        99059.0|           0|                1|Normal|  2000|        65074|                 65074|\n",
      "| 10.12667110305036|                  26|  13|          3|         0|      180|           14700.0|       441203.0|           0|                1|Normal| 25000|        04202|                 04202|\n",
      "|  4.61512051684126|                 565|  12|          1|         0|       91|            4700.0|       274515.0|           0|                1|Normal|   100|        51110|                 51110|\n",
      "| 9.903537551286169|                 738|   7|          4|         0|      254|14333.333333333334|        65765.0|           0|                1|Normal| 20000|        65784|                 65784|\n",
      "|6.2166061010848646|                 391|  14|          6|         1|        6| 823.3333333333334|       169045.0|           0|                1|Normal|   500|        36560|                 36560|\n",
      "| 8.036249942132116|                 741|  13|          1|         0|       32|2863.3333333333335|        56217.0|           0|                1|Normal|  3090|        65802|                 65802|\n",
      "| 9.903537551286169|                 564|  15|          1|         0|      228|10066.666666666666|      2097912.0|           0|                1|Normal| 20000|        51030|                 51030|\n",
      "| 7.601402334583733|                 658|  21|          6|         1|       62|            2000.0|       883908.0|           0|                1|Normal|  2000|        59594|                 59594|\n",
      "+------------------+--------------------+----+-----------+----------+---------+------------------+---------------+------------+-----------------+------+------+-------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the original AccountNumber values (ensure this is the same data used for encoding)\n",
    "le.fit(df['AccountNumber'])\n",
    "\n",
    "# Inverse transform the encoded values to get the original AccountNumber\n",
    "df['AccountNumber_original'] = le.inverse_transform(df['AccountNumberEncoded'])\n",
    "# Merge test_data with df on the index to align the rows\n",
    "result_df = test_data.copy()\n",
    "result_df['Amount'] = df.loc[result_df.index, 'Amount']\n",
    "result_df['AccountNumber'] = df.loc[result_df.index, 'AccountNumber']\n",
    "result_df['AccountNumber_original'] = df.loc[result_df.index, 'AccountNumber_original']\n",
    "\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AnomalyDetection\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Convert to PySpark DataFrame\n",
    "spark_df = spark.createDataFrame(result_df)\n",
    "\n",
    "# Cast 'Amount' to integer type\n",
    "spark_df = spark_df.withColumn('Amount', col('Amount').cast('int'))\n",
    "spark_df.write \\\n",
    "    .mode('overwrite') \\\n",
    "    .option('overwriteSchema', 'true') \\\n",
    "    .saveAsTable('PesaTransactionsV1.Anomaly_Results')\n",
    "# Verify the data in the table\n",
    "spark.sql(\"SELECT * FROM PesaTransactionsV1.Anomaly_Results\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d46a0ab9-a1bd-4d4d-83dd-f3886625e5f6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-04-08T19:49:13.9464973Z",
       "execution_start_time": "2025-04-08T19:49:04.207855Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "d5d9375f-04ec-4e80-abb0-27043b96c099",
       "queued_time": "2025-04-08T19:48:44.7334133Z",
       "session_id": "ac3cf0cc-e561-4c55-aa92-182d3645a533",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 75,
       "statement_ids": [
        75
       ]
      },
      "text/plain": [
       "StatementMeta(, ac3cf0cc-e561-4c55-aa92-182d3645a533, 75, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+----+-----------+----------+---------+------------------+---------------+------------+-----------------+------+-------------+----------------------+------+\n",
      "|        log_amount|AccountNumberEncoded|hour|day_of_week|is_weekend|txn_count|   rolling_avg_amt|time_since_last|is_first_txn|predicted_anomaly|Amount|AccountNumber|AccountNumber_original|Status|\n",
      "+------------------+--------------------+----+-----------+----------+---------+------------------+---------------+------------+-----------------+------+-------------+----------------------+------+\n",
      "|10.308985993422082|                 494|  15|          2|         0|       29|14533.333333333334|      1821633.0|           0|                1| 30000|        44324|                 44324|Normal|\n",
      "| 6.803505257608338|                 848|  10|          1|         0|      136|            1400.0|      2129169.0|           0|                1|   900|        74804|                 74804|Normal|\n",
      "| 7.438971592395862|                 291|  15|          6|         1|      146|            8400.0|      1546538.0|           0|                1|  1700|        26842|                 26842|Normal|\n",
      "| 9.210440366976515|                 548|   8|          0|         0|       77|            6000.0|        68252.0|           0|                1| 10000|        49302|                 49302|Normal|\n",
      "| 9.105090961257085|                 749|   8|          4|         0|       57|3336.6666666666665|       576416.0|           0|                1|  9000|        66826|                 66826|Normal|\n",
      "| 6.685860947068359|                 204|  18|          1|         0|       42| 933.3333333333334|       124639.0|           0|                1|   800|        20082|                 20082|Normal|\n",
      "| 8.006700845440367|                 631|   9|          4|         0|      173|           10260.0|      1549384.0|           0|                1|  3000|        57152|                 57152|Normal|\n",
      "|10.308985993422082|                1068|   0|          2|         0|      201|21633.333333333332|        84319.0|           0|                1| 30000|        92850|                 92850|Normal|\n",
      "|10.308985993422082|                 389|  12|          4|         0|       16|           10250.0|      4148248.0|           0|                1| 30000|        36396|                 36396|Normal|\n",
      "| 6.746412128573374|                 487|  19|          0|         0|      153|1026.6666666666667|       128104.0|           0|                1|   850|        43818|                 43818|Normal|\n",
      "|10.308985993422082|                1111|  18|          0|         0|      145|29233.333333333332|       874708.0|           0|                1| 30000|        96864|                 96864|Normal|\n",
      "|10.308985993422082|                 306|  12|          5|         1|       86|           30000.0|       103014.0|           0|                1| 30000|        28446|                 28446|Normal|\n",
      "| 7.601402334583733|                 723|  13|          6|         1|      155|            2000.0|        99059.0|           0|                1|  2000|        65074|                 65074|Normal|\n",
      "| 10.12667110305036|                  26|  13|          3|         0|      180|           14700.0|       441203.0|           0|                1| 25000|        04202|                 04202|Normal|\n",
      "|  4.61512051684126|                 565|  12|          1|         0|       91|            4700.0|       274515.0|           0|                1|   100|        51110|                 51110|Normal|\n",
      "| 9.903537551286169|                 738|   7|          4|         0|      254|14333.333333333334|        65765.0|           0|                1| 20000|        65784|                 65784|Normal|\n",
      "|6.2166061010848646|                 391|  14|          6|         1|        6| 823.3333333333334|       169045.0|           0|                1|   500|        36560|                 36560|Normal|\n",
      "| 8.036249942132116|                 741|  13|          1|         0|       32|2863.3333333333335|        56217.0|           0|                1|  3090|        65802|                 65802|Normal|\n",
      "| 9.903537551286169|                 564|  15|          1|         0|      228|10066.666666666666|      2097912.0|           0|                1| 20000|        51030|                 51030|Normal|\n",
      "| 7.601402334583733|                 658|  21|          6|         1|       62|            2000.0|       883908.0|           0|                1|  2000|        59594|                 59594|Normal|\n",
      "+------------------+--------------------+----+-----------+----------+---------+------------------+---------------+------------+-----------------+------+-------------+----------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If there status column is already in the table, first drop it. (Overwriting wouldnt work)\n",
    "\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Drop the 'status' column in the Spark DataFrame if it exists\n",
    "if \"Status\" in spark_df.columns:\n",
    "    spark_df = spark_df.drop(\"Status\")\n",
    "\n",
    "# Add the new 'status' column based on 'predicted_anomaly'\n",
    "spark_df = spark_df.withColumn(\"Status\", \n",
    "                               when(col(\"predicted_anomaly\") == 0, \"Anomaly\")\n",
    "                               .otherwise(\"Normal\"))\n",
    "\n",
    "\n",
    "# Overwrite the data in the target lakehouse table\n",
    "spark_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"PesaTransactionsV1.Anomaly_Results\")\n",
    "\n",
    "\n",
    "# Verify the data in the table\n",
    "spark.sql(\"SELECT * FROM PesaTransactionsV1.Anomaly_Results\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1d8f7c25-c885-4f13-8303-ea7fcf2ac250",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-04-08T19:49:14.7678675Z",
       "execution_start_time": "2025-04-08T19:49:13.9487068Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "986424d4-01bc-424a-ac0b-0140ef5b5e14",
       "queued_time": "2025-04-08T19:48:44.8027763Z",
       "session_id": "ac3cf0cc-e561-4c55-aa92-182d3645a533",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 76,
       "statement_ids": [
        76
       ]
      },
      "text/plain": [
       "StatementMeta(, ac3cf0cc-e561-4c55-aa92-182d3645a533, 76, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter out anomaly rows\n",
    "import pandas as pd\n",
    "\n",
    "df_anomalies = spark.read.table(\"Anomaly_Results\").filter(\"Status = 'Anomaly'\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d2535e8a-8a29-4641-b3dd-b6be16ba6a0d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-04-08T19:49:15.0520288Z",
       "execution_start_time": "2025-04-08T19:49:14.7699382Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "96f3377e-65a0-48c8-b8e9-30ace373e841",
       "queued_time": "2025-04-08T19:48:44.8952016Z",
       "session_id": "ac3cf0cc-e561-4c55-aa92-182d3645a533",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 77,
       "statement_ids": [
        77
       ]
      },
      "text/plain": [
       "StatementMeta(, ac3cf0cc-e561-4c55-aa92-182d3645a533, 77, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['log_amount', 'AccountNumberEncoded', 'hour', 'day_of_week', 'is_weekend', 'txn_count', 'rolling_avg_amt', 'time_since_last', 'is_first_txn', 'predicted_anomaly', 'Amount', 'AccountNumber', 'AccountNumber_original', 'Status']\n"
     ]
    }
   ],
   "source": [
    "# Verify the DataFrame columns:\n",
    "print(df_anomalies.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40b5991-8a72-48f1-bf68-4a8857388797",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### **STEP 2:** Use OpenAI GPT4o-mini to analyze the anomaly level (low/medium/high), along with explanations to the deductions and the message that will be sent to the risk analysts.\n",
    "\n",
    "- At this point, we've already created a Key Vault on the Azure portal to store the OpenAI endpoint-url and the OpenAI key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "66ee0742-869d-46b0-b930-8108d4beb876",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-04-08T19:49:15.3339676Z",
       "execution_start_time": "2025-04-08T19:49:15.0539639Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "0e5a53c3-55dc-4626-8a98-8a547b8b2535",
       "queued_time": "2025-04-08T19:48:45.0397125Z",
       "session_id": "ac3cf0cc-e561-4c55-aa92-182d3645a533",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 78,
       "statement_ids": [
        78
       ]
      },
      "text/plain": [
       "StatementMeta(, ac3cf0cc-e561-4c55-aa92-182d3645a533, 78, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to generate the prompt to send to GPT4 for each anomaly\n",
    "def format_prompt(transaction):\n",
    "    return f\"\"\"\n",
    "        A transaction has been flagged by an anomaly detection system. \n",
    "        Details:\n",
    "        - Anomaly: {transaction[\"Status\"]}\n",
    "        - Amount: KES{transaction[\"Amount\"]}\n",
    "        - Account No: {transaction[\"AccountNumber\"]}\n",
    "\n",
    "        Evaluate this transaction and assign a risk level (low, medium, high) with\n",
    "        a short explanation. Provide a final alert message in plain English for a \n",
    "        fraud analyst.\n",
    "\n",
    "        Add three columns for your output:\n",
    "        1. RiskLevel.\n",
    "        2. Explanation.\n",
    "        3. AlertMessage\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2410eb8-376c-4949-a902-3b554a800469",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Run this in a notebook cell before your imports\n",
    "!pip install openai==0.28.1 typing-extensions==4.5.0 pydantic==1.10.13 notebookutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "43fdc5b4-12ba-4dff-9497-3c43a2f9297e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-04-08T19:49:20.9397556Z",
       "execution_start_time": "2025-04-08T19:49:20.1493354Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "d33d4993-4259-4b00-9b78-2db3c9998b0f",
       "queued_time": "2025-04-08T19:48:45.4085032Z",
       "session_id": "ac3cf0cc-e561-4c55-aa92-182d3645a533",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 80,
       "statement_ids": [
        80
       ]
      },
      "text/plain": [
       "StatementMeta(, ac3cf0cc-e561-4c55-aa92-182d3645a533, 80, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call GPT4o mini\n",
    "import openai\n",
    "from notebookutils.mssparkutils.credentials import getSecret\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "# Constants\n",
    "KEY_VAULT_ENDPOINT = \"https://pesatransactionskeyvault.vault.azure.net/\"\n",
    "AZURE_OPENAI_API_KEY = getSecret(KEY_VAULT_ENDPOINT, \"openaiGPT4o-mini-key\")\n",
    "AZURE_OPENAI_ENDPOINT_URL = getSecret(KEY_VAULT_ENDPOINT, \"openaiendpointURL\")\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = AZURE_OPENAI_ENDPOINT_URL\n",
    "openai.api_version = \"2025-03-01-preview\"\n",
    "openai.api_key = AZURE_OPENAI_API_KEY\n",
    "\n",
    "def gpt4o_risk_evaluation(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=\"gpt-4o-mini-kenya-hack\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"\"You are a fraud assistant. Always respond in valid JSON format with these exact keys:\n",
    "            {\n",
    "                \"risk_level\": \"low/medium/high\",\n",
    "                \"explanation\": \"your explanation here\",\n",
    "                \"alert_message\": \"your alert message here\"\n",
    "            }\"\"\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return json.loads(response.choices[0].message[\"content\"])\n",
    "\n",
    "def parse_gpt4_response(response_text):\n",
    "    \"\"\"Parse the GPT-4 response into structured components\"\"\"\n",
    "    risk_level = response_text[\"risk_level\"]\n",
    "    explanation = response_text[\"explanation\"]\n",
    "    alert_message = response_text[\"alert_message\"]\n",
    "    \n",
    "    return pd.Series({\n",
    "        'RiskLevel': risk_level,\n",
    "        'Explanation': explanation,\n",
    "        'AlertMessage': alert_message\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "819397df-ebd3-4fa8-bebf-23a482b0f889",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-04-08T19:49:27.0335177Z",
       "execution_start_time": "2025-04-08T19:49:20.941906Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "4d26f661-aff9-4875-aa13-ee3c83e127e1",
       "queued_time": "2025-04-08T19:48:45.4927575Z",
       "session_id": "ac3cf0cc-e561-4c55-aa92-182d3645a533",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 81,
       "statement_ids": [
        81
       ]
      },
      "text/plain": [
       "StatementMeta(, ac3cf0cc-e561-4c55-aa92-182d3645a533, 81, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_amount</th>\n",
       "      <th>AccountNumberEncoded</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>txn_count</th>\n",
       "      <th>rolling_avg_amt</th>\n",
       "      <th>time_since_last</th>\n",
       "      <th>is_first_txn</th>\n",
       "      <th>predicted_anomaly</th>\n",
       "      <th>Amount</th>\n",
       "      <th>AccountNumber</th>\n",
       "      <th>AccountNumber_original</th>\n",
       "      <th>Status</th>\n",
       "      <th>RiskLevel</th>\n",
       "      <th>Explanation</th>\n",
       "      <th>AlertMessage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.615872</td>\n",
       "      <td>260</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>15200.000000</td>\n",
       "      <td>11316140.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15000</td>\n",
       "      <td>24498</td>\n",
       "      <td>24498</td>\n",
       "      <td>Anomaly</td>\n",
       "      <td>medium</td>\n",
       "      <td>The transaction amount of KES15000 is signific...</td>\n",
       "      <td>A transaction of KES15000 has been flagged as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.294300</td>\n",
       "      <td>1044</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>2333.333333</td>\n",
       "      <td>14991315.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4000</td>\n",
       "      <td>91126</td>\n",
       "      <td>91126</td>\n",
       "      <td>Anomaly</td>\n",
       "      <td>medium</td>\n",
       "      <td>The transaction amount of KES4000 is moderate ...</td>\n",
       "      <td>A transaction of KES4000 from account number 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.308986</td>\n",
       "      <td>1138</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30000</td>\n",
       "      <td>98588</td>\n",
       "      <td>98588</td>\n",
       "      <td>Anomaly</td>\n",
       "      <td>medium</td>\n",
       "      <td>The transaction amount of KES30000 is signific...</td>\n",
       "      <td>Please review the transaction of KES30000 on a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   log_amount  AccountNumberEncoded  hour  day_of_week  is_weekend  txn_count  \\\n",
       "0    9.615872                   260    16            5           1         69   \n",
       "1    8.294300                  1044    18            5           1         68   \n",
       "2   10.308986                  1138    15            1           0         32   \n",
       "\n",
       "   rolling_avg_amt  time_since_last  is_first_txn  predicted_anomaly  Amount  \\\n",
       "0     15200.000000       11316140.0             0                  0   15000   \n",
       "1      2333.333333       14991315.0             0                  0    4000   \n",
       "2     30000.000000             -1.0             1                  0   30000   \n",
       "\n",
       "  AccountNumber AccountNumber_original   Status RiskLevel  \\\n",
       "0         24498                  24498  Anomaly    medium   \n",
       "1         91126                  91126  Anomaly    medium   \n",
       "2         98588                  98588  Anomaly    medium   \n",
       "\n",
       "                                         Explanation  \\\n",
       "0  The transaction amount of KES15000 is signific...   \n",
       "1  The transaction amount of KES4000 is moderate ...   \n",
       "2  The transaction amount of KES30000 is signific...   \n",
       "\n",
       "                                        AlertMessage  \n",
       "0  A transaction of KES15000 has been flagged as ...  \n",
       "1  A transaction of KES4000 from account number 9...  \n",
       "2  Please review the transaction of KES30000 on a...  "
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loop through the anomalies and evaluate each\n",
    "\"\"\"\n",
    "- The intention for this is to come up with a table with the original data, but with additional columns for the risk level,\n",
    "the explanation as to why this is an anomaly, and the alert message to be sent to analysts.\n",
    "\"\"\"\n",
    "\n",
    "response_series = df_anomalies.apply(\n",
    "    lambda row: parse_gpt4_response(gpt4o_risk_evaluation(format_prompt(row))), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Add the parsed columns to your DataFrame\n",
    "df_anomalies[['RiskLevel', 'Explanation', 'AlertMessage']] = response_series\n",
    "\n",
    "# Show what we have so far:\n",
    "df_anomalies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "49948a66-d705-49cc-8ed7-10af88d7cef0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-04-08T19:49:34.7775255Z",
       "execution_start_time": "2025-04-08T19:49:27.0356533Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "a9452add-c518-41b9-a241-918f722c6f04",
       "queued_time": "2025-04-08T19:48:45.5800697Z",
       "session_id": "ac3cf0cc-e561-4c55-aa92-182d3645a533",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 82,
       "statement_ids": [
        82
       ]
      },
      "text/plain": [
       "StatementMeta(, ac3cf0cc-e561-4c55-aa92-182d3645a533, 82, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save results back to the lakehouse but in a new table\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_df = spark.createDataFrame(df_anomalies)\n",
    "\n",
    "spark_df.write.format(\"delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"Anomaly_Risk_Summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e137f12-7499-49a9-935a-567061960152",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### **STEP 3:** Connect to Azure blob storage, upload `.txt` files (the text in here is the alert message content with crucial info) for any anomalies found.\n",
    "\n",
    "- At this point, a Storage Account is already created on Azure, along with the blob container that we'll use to store the uploaded files.\n",
    "- The final cell in this section will log; A message to show if connecting to the blob container is successful, if uploading the anomaly files is successful, and lastly, a verification to show how many items are currently in the blob container (well, only a few file names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26038438-3efc-4588-98be-8a35509154de",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Install Dependencies\n",
    "!pip install azure-storage-blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43563e7-35ac-4d8d-899e-6c3efbff3a42",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "## Save Anomalies Summary to Blob Storage\n",
    "# This cell handles exporting our processed data to Azure Blob Storage as JSON\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Constants\n",
    "KEY_VAULT_ENDPOINT = \"https://pesatransactionskeyvault.vault.azure.net/\"\n",
    "AZURE_OPENAI_API_KEY = getSecret(KEY_VAULT_ENDPOINT, \"openaiGPT4o-mini-key\")\n",
    "AZURE_OPENAI_ENDPOINT_URL = getSecret(KEY_VAULT_ENDPOINT, \"openaiendpointURL\")\n",
    "\n",
    "# Configuration Section\n",
    "STORAGE_ACCOUNT_NAME = getSecret(KEY_VAULT_ENDPOINT, \"storageAccountName\")\n",
    "#STORAGE_ACCOUNT_KEY = getSecret(KEY_VAULT_ENDPOINT, \"storageAccountKey\")\n",
    "STORAGE_ACCOUNT_KEY = \"7DH6ouC5LVbOa0hjo3zjI0JHq9hxk+dJPAFJYlUNpRpESnl5T2P5+2BRilkyZVN1lzNJnWJvnmPLV+ASt8JuRaQ==\"\n",
    "CONTAINER_NAME = \"gpt4osummary\"\n",
    "\n",
    "# Generate timestamp for filename\n",
    "TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "BLOB_NAME = f\"anomaly_alerts_{TIMESTAMP}.txt\"\n",
    "\n",
    "# Create the connection to Azure Blob Storage\n",
    "try:\n",
    "    connection_string = f\"DefaultEndpointsProtocol=https;AccountName={STORAGE_ACCOUNT_NAME};\\\n",
    "        AccountKey={STORAGE_ACCOUNT_KEY};EndpointSuffix=core.windows.net\"\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "    \n",
    "    # Get reference to container (will create if doesn't exist)\n",
    "    container_client = blob_service_client.get_container_client(CONTAINER_NAME)\n",
    "    if not container_client.exists():\n",
    "        container_client.create_container()\n",
    "    \n",
    "    print(f\"Successfully connected to Azure Blob Storage container: {CONTAINER_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Azure Blob Storage: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Convert Spark DataFrame to text files and upload\n",
    "try:\n",
    "    # First convert Spark DataFrame to Pandas\n",
    "    pandas_df = spark_df.toPandas()\n",
    "    \n",
    "    # Ensure the required column exists\n",
    "    if 'AlertMessage' not in pandas_df.columns:\n",
    "        raise ValueError(\"DataFrame is missing required 'AlertMessage' column\")\n",
    "    \n",
    "    # Upload each summary as a separate text file\n",
    "    for index, row in pandas_df.iterrows():\n",
    "        # Create filename with timestamp and index\n",
    "        txt_filename = f\"anomaly_summary_{TIMESTAMP}_{index}.txt\"\n",
    "        \n",
    "        # Get the summary text\n",
    "        summary_text = str(row['AlertMessage'])\n",
    "        \n",
    "        # Upload to blob storage\n",
    "        blob_client = container_client.get_blob_client(txt_filename)\n",
    "        blob_client.upload_blob(summary_text, overwrite=True)\n",
    "        \n",
    "        print(f\"Uploaded: {txt_filename} (Size: {len(summary_text)} characters)\")\n",
    "    \n",
    "    print(f\"\\nSuccessfully uploaded {len(pandas_df)} text files to container {CONTAINER_NAME}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading text files to blob storage: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Verification - list blobs in container (optional)\n",
    "print(\"\\nCurrent blobs in container:\")\n",
    "try:\n",
    "    blob_list = container_client.list_blobs()\n",
    "    txt_files = [blob for blob in blob_list if blob.name.endswith('.txt')]\n",
    "    \n",
    "    print(f\"Found {len(txt_files)} text files:\")\n",
    "    for blob in txt_files[:5]:\n",
    "        print(f\"- {blob.name} (Size: {blob.size} bytes)\")\n",
    "    if len(txt_files) > 5:\n",
    "        print(f\"- ... and {len(txt_files)-5} more\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error listing blobs: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "04ad2f29-2adf-4cb3-891c-174254ca83a8",
    "default_lakehouse_name": "PesaTransactionsV1",
    "default_lakehouse_workspace_id": "b6ab2d30-caf0-43d2-ad6a-165345f8b178",
    "known_lakehouses": [
     {
      "id": "04ad2f29-2adf-4cb3-891c-174254ca83a8"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
